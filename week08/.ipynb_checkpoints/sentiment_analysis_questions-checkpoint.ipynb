{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis\n",
    "In this exercise, we will classify the sentiment of text documents. Complete the code with TODO tag.\n",
    "\n",
    "References and Further Readings:\n",
    "+ http://www.nltk.org/howto/sentiment.html\n",
    "+ https://www.nltk.org/api/nltk.sentiment.html\n",
    "+ http://datameetsmedia.com/vader-sentiment-analysis-explained/\n",
    "+ https://github.com/cjhutto/vaderSentiment\n",
    "+ https://marcobonzanini.com/2015/05/17/mining-twitter-data-with-python-part-6-sentiment-analysis-basics/\n",
    "+ https://github.com/marrrcin/ml-twitter-sentiment-analysis\n",
    "\n",
    "\n",
    "### 2.1. Classification approach\n",
    "\n",
    "Classification approach looks at previously labeled data in order to determine the sentiment of never-before-seen sentences. It involves training a model using previously seen text to predict/classify the sentiment of some new input text. The nice thing is that, with a greater volume of data, we generally get better prediction or classification results. However, unlike the lexical approach, we need previously labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\s5094000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\s5094000\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "# nltk.download('subjectivity')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "pos_docs = [(list(movie_reviews.words(pos_id)), 'pos') for pos_id in movie_reviews.fileids('pos')[:]]\n",
    "neg_docs = [(list(movie_reviews.words(neg_id)), 'neg') for neg_id in movie_reviews.fileids('neg')[:]]\n",
    "len(pos_docs), len(neg_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separately split subjective and objective instances to keep a balanced uniform class distribution in both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films', 'adapted', 'from', 'comic', 'books']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: split training and testing data as 80/20\n",
    "train_pos_docs = pos_docs[:int(len(pos_docs) * 0.8)]\n",
    "test_pos_docs = pos_docs[int(len(pos_docs) * 0.8):]\n",
    "train_neg_docs = neg_docs[:int(len(neg_docs) * 0.8)]\n",
    "test_neg_docs = neg_docs[int(len(neg_docs) * 0.8):]\n",
    "\n",
    "training_docs = train_pos_docs + train_neg_docs\n",
    "testing_docs = test_pos_docs + test_neg_docs\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "\n",
    "all_words_neg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use simple unigram word features, handling negation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2721\n"
     ]
    }
   ],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "print(len(unigram_feats))\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply features to obtain a feature-value representation of our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "# print(training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our classifier on the training set, and subsequently output the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use Naive Bayes to train the sentiment classifier\n",
    "naive_bayes = NaiveBayesClassifier.train\n",
    "# svm = SklearnClassifier(LinearSVC()).train\n",
    "# maxent = MaxentClassifier.train\n",
    "classifier = sentim_analyzer.train(naive_bayes, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_items([('Accuracy', 0.7641304347826087), ('Precision [pos]', 0.7773972602739726), ('Recall [pos]', 0.7402173913043478), ('F-measure [pos]', 0.7583518930957683), ('Precision [neg]', 0.7520746887966805), ('Recall [neg]', 0.7880434782608695), ('F-measure [neg]', 0.7696390658174097)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentim_analyzer.evaluate(test_set).items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Lexical approach\n",
    "\n",
    "Lexical approaches aim to map words to sentiment by building a lexicon or a 'dictionary of sentiment'. We can use this dictionary to assess the sentiment of phrases and sentences, without the need of looking at anything else. Sentiment can be categorical – such as {negative, neutral, positive} – or it can be numerical – like a range of intensities or scores. Lexical approaches look at the sentiment category or score of each word in the sentence and decide what the sentiment category or score of the whole sentence is. The power of lexical approaches lies in the fact that we do not need to train a model using labeled data, since we have everything we need to assess the sentiment of sentences in the dictionary of emotions. VADER is an example of a lexical method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the lexical approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for doc in testing_docs:\n",
    "    doc = \" \".join(doc[0])\n",
    "#     print(doc[:100] + \"...\")\n",
    "    ss = sid.polarity_scores(doc)\n",
    "    for k in sorted(ss):\n",
    "        pass\n",
    "#         print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparing two approaches\n",
    "\n",
    "First we can transform the sentiment score by the lexical approach into label by the following rules:\n",
    "\n",
    "+ positive sentiment: compound score > 0\n",
    "+ negative sentiment: compound score <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just how inseparable is the team of sgt . martin riggs ( mel gibson ) and sgt . roger murtaugh ( dan... pos\n",
      "since 1990 , the dramatic picture has undergone a certain change of style . now , instead of emphasi... pos\n",
      "a big surprise to me . the good trailer had hinted that they pulled the impossible off , but making ... pos\n",
      "after having heard so many critics describe \" return to me \" as an old - fashioned hollywood romance... pos\n",
      "wild things is a suspenseful thriller starring matt dillon , denise richards , and neve campbell tha... pos\n",
      "* * * * * * minor plot spoilers in review * * * * * * * * * * * * no major spoilers are in review * ... pos\n",
      "are you tired of all the hot new releases being gone by the time you get to the video store ? waffle... neg\n",
      "many people dislike french films for their lack of closure . while possibly shallow , i ' ve often h... pos\n",
      "the keen wisdom of an elderly bank robber , the naive ambitions of a sexy hospital nurse , and a par... pos\n",
      "robert benton has assembled a stellar , mature cast for his latest feature , twilight , a film noir ... pos\n",
      "warning : anyone offended by blatant , leering machismo had better avoid this film . or lots of bloo... neg\n",
      "accepting his oscar as producer of this year ' s best picture winner , saul zaentz remarked that his... pos\n",
      "a bleak look at how the boston underworld operates , a film which was based on the best - seller by ... neg\n",
      "\" footloose \" has only one goal in mind : to reel in an audience with cheesy sentiment and feel - go... pos\n",
      "as i walked out of crouching tiger , hidden dragon i thought to myself that i had had just seen a gr... pos\n",
      "\" crazy / beautiful \" suffers from the damned - if - you - do , damned - if - you - don ' t syndrome... pos\n",
      "everyone knows someone like giles de ' ath : stuffy , arrogant , set in his ways , and at war with a... pos\n",
      "for many people , procrastination isn ' t a problem to overcome , it ' s a high art . we ' ll do jus... neg\n",
      "meet joe black ( reviewed on nov . 27 / 98 ) starring brad pitt , anthony hopkins , claire forlani i... neg\n",
      "call it touched by a demon . gregory hoblit ' s fallen is a serial killer movie with an unusual , ho... neg\n",
      "james l . brooks , one of the developers of the simpsons and director of broadcast news , returns to... pos\n",
      "after a stylistic detour with mrs . parker and the vicious circle ( which , despite its uncomfortabl... pos\n",
      "\" the fugitive \" is probably one of the greatest thrillers ever made . it takes realistic , believab... pos\n",
      "i think that any competent member of the human race who ' s ever seen a movie -- any movie -- could ... pos\n",
      "based on the boris karloff ' s classic by the same name , the mummy starts off with the high - pries... pos\n",
      "on re - watching italian writer / director dario argento ' s much lauded murder mystery tenebrae , i... neg\n",
      "as any sociologist will attest , your childhood and the types of parents that you have will heavily ... neg\n",
      "is evil dead ii a bad movie ? it ' s full of terrible acting , pointless violence , and plot holes y... neg\n",
      "it ' s rather strange too have two computer animated talking ant movies come out in a single year , ... pos\n",
      "not since oliver stone ' s natural born killers has there been a movie this incendiary , and not sin... pos\n",
      "plot : a bunch of bad guys dressed up as elvis impersonators rob a vegas casino during a presley con... pos\n",
      "> from the commercials , this looks like a mild - mannered neil simonesque tale with mary tyler moor... pos\n",
      "i ' m not quite sure how best to go about writing this review . i must admit that i was a little dis... neg\n",
      "the sheer horrific audacity of the nazi plan to exterminate the jews of europe is almost incomprehen... pos\n",
      "it is easy to label something sentimental or tear jerking when one has not experienced the heartache... pos\n",
      "i know that \" funnest \" isn ' t a word . \" fun \" is a noun , and therefore cannot be conjugated like... pos\n",
      "capsule : a short punchy action sequel to the two dinosaur films made by steven spielberg . joe john... pos\n",
      "just look back two years ago at the coen brothers ' comedic gem the big lebowski , change the actors... neg\n",
      "plot : a human space astronaut accidentally falls upon a planet ruled by apes . he is taken prisoner... pos\n",
      "disaster films have a tendency to be very formulated and very cliched . to see a disaster film with ... pos\n",
      "in brief : best bleak comedy film since heathers . full of wonderful swipes at the fifteen minutes o... pos\n",
      "i relish those rare opportunities when a talented screenwriter can make me feel like a fool . i spen... pos\n",
      "warning : this review contains some spoilers for the 1964 film \" fail - safe . \" if you haven ' t se... neg\n",
      "andrew lloyd webber ' s musicals , preferably his early work with lyricist tim rice , present a musi... pos\n",
      "there ' s more to a quilt than fabric and thread -- each patchwork design has its own unique story (... pos\n",
      "mimi leder is probably best known for her stunning work as a director of the hit tv - show er . her ... pos\n",
      "i have to say it . tim burton ' s retelling of planet of the apes is more fun than a barrel of monke... neg\n",
      "urban legend surprised me . based on the last few films the genre has produced ( including but not e... pos\n",
      "capsule : the verma family is having a wedding and all the relatives will come for the multi - day f... pos\n",
      "the love for family is one of the strongest driving forces in any man ' s life , especially for thos... pos\n"
     ]
    }
   ],
   "source": [
    "def lexical_sentiment(doc, sid=None):\n",
    "    \"\"\"TODO: return the label 'pos' or 'neg' for a document\"\"\"\n",
    "    if sid is None:\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(doc)\n",
    "    if ss[\"compound\"] > 0:\n",
    "        return 'pos'\n",
    "    return 'neg'\n",
    "\n",
    "for doc in testing_docs[:50]:\n",
    "    doc = \" \".join(doc[0])\n",
    "    label = lexical_sentiment(doc, sid)\n",
    "    print(doc[:100] + \"...\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the lexical approach by computing accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.metrics import (accuracy as eval_accuracy, precision as eval_precision,\n",
    "        recall as eval_recall, f_measure as eval_f_measure)\n",
    "\n",
    "gold_results = defaultdict(set)\n",
    "test_results = defaultdict(set)\n",
    "acc_gold_results = []\n",
    "acc_test_results = []\n",
    "labels = set()\n",
    "num = 0\n",
    "for i, (text, label) in enumerate(testing_docs):\n",
    "    labels.add(label)\n",
    "    gold_results[label].add(i)\n",
    "    acc_gold_results.append(label)\n",
    "    observed = lexical_sentiment(\" \".join(text), sid)\n",
    "    num += 1\n",
    "    acc_test_results.append(observed)\n",
    "    test_results[observed].add(i)\n",
    "metrics_results = {}\n",
    "\n",
    "# TODO: compute the accuracy metrics\n",
    "metrics_results[\"Accuracy\"] = eval_accuracy(gold_results, test_results)\n",
    "for label in labels:\n",
    "\n",
    "for result in sorted(metrics_results):\n",
    "        print('{0}: {1}'.format(result, metrics_results[result]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
